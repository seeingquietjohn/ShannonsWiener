{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4460b153-0e82-4590-a97f-d72a934e6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log, e\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2924f103-24d5-4485-b340-08b6af3c95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobnorm(mat): \n",
    "    r\"\"\"\n",
    "        Compute the frobenius norm \n",
    "    \"\"\"\n",
    "    n, m = len(mat), len(mat[0])\n",
    "    matsum = 0\n",
    "    for i in range(0,n): \n",
    "        for j in range(0,m): \n",
    "            matsum += mat[i][j]**2\n",
    "\n",
    "    return matsum**0.5\n",
    "\n",
    "\n",
    "def cossim(a,b): \n",
    "    r\"\"\"\n",
    "        Compute the cossine similarity\n",
    "    \"\"\"\n",
    "    return np.trace(a.T @ b) / (frobnorm(a)*frobnorm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16f4df37-9997-40b0-ab4a-58099b5859b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ell_inf(A, B): # chebyshev norm\n",
    "    return max(abs(a-b) for a,b in zip(A,B))\n",
    "\n",
    "def cor_sum(vec, r): # correlation sum\n",
    "    r\"\"\"\n",
    "        Compute the correlation sum given some radius r\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(vec)\n",
    "    neighbor_counts = [\n",
    "        sum(ell_inf(x_i, x_j) <= r for x_j in vec) / n\n",
    "        for x_i in vec\n",
    "    ]\n",
    "\n",
    "    return sum(neighbor_counts) / n\n",
    "\n",
    "def approx_entropy(time_series, m, d, r): \n",
    "    \"\"\"\n",
    "        Estimate the approximate/discrete entropy of a time series (approximating the growth rate of r-balls). \n",
    "\n",
    "        Params: \n",
    "\n",
    "        time_series: 1d array_like \n",
    "        m, d: int\n",
    "            delay embedding parameters\n",
    "        r: float\n",
    "            radius determining the size of an r-ball\n",
    "    \"\"\"\n",
    "    \n",
    "    def phi(delay_dim, delay): \n",
    "        \"\"\"\n",
    "            Compute the log-average of r-balls assuming the delay embedding parameters result in a diffeomorphism \n",
    "            of the 1d time series from R^1 to R^d. \n",
    "        \"\"\"\n",
    "        X = delay_embed(time_series, delay_dim, delay)\n",
    "        n = len(X)\n",
    "        neighbor_counts = [\n",
    "            sum(ell_inf(x_i, x_j) <= r for x_j in X) / n\n",
    "            for x_i in X\n",
    "        ]\n",
    "        count = [c for c in neighbor_counts if c > 0]\n",
    "        return sum(math.log(c) for c in count) / n\n",
    "\n",
    "    return abs(phi(m+1, d) - phi(m, d))\n",
    "\n",
    "def cor_dim(time_series, m, d, r): # correlation dimension; easier to compute than minkowski dimension\n",
    "    \"\"\"\n",
    "        Estimate the correlation dimension of a time series\n",
    "    \"\"\"\n",
    "    X = delay_embed(time_series, m, d)\n",
    "    CS = cor_sum(X, r)\n",
    "\n",
    "    return math.log(CS) / math.log(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e805bd99-5d5f-4c67-91a3-4ea22da15493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def AutoCor_k(arr, k): \n",
    "#     n = len(arr)\n",
    "#     M = np.mean(arr)\n",
    "#     num, denom = 0, 0\n",
    "#     for i in range(0, n):\n",
    "#         denom+=(arr[i]-M)**2\n",
    "#     for i in range(0, n-k): \n",
    "#         num+=(arr[i]-M)*(arr[i+k]-M)\n",
    "\n",
    "#     return num/denom\n",
    "\n",
    "def acf(x, lag, unbiased=True): # autocorrelation function; used to determine the optimal time delay\n",
    "    r\"\"\"\n",
    "        Compute the autocorrelation of a signal given delay/lag k. Can be used to determine the optimal time delay. \n",
    "\n",
    "        Params:\n",
    "\n",
    "        x: 1d array_like input data\n",
    "        lag: integer \n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    x = x-x.mean()\n",
    "    n = x.size\n",
    "\n",
    "    num = np.dot(x[:n-lag], x[lag:])\n",
    "    denom = np.dot(x, x)\n",
    "\n",
    "    if unbiased: \n",
    "        num /= (n-lag)\n",
    "        denom /= n\n",
    "    else: \n",
    "        num /= n\n",
    "        denom /= n\n",
    "\n",
    "    return num / denom\n",
    "\n",
    "def plot_acf(x, max_lag, unbiased=True): \n",
    "    r\"\"\"\n",
    "        Plot ACF from delay/lag = 1 to max_lag k. Returns the list [acf_i] where i is the delay parameter. \n",
    "\n",
    "        Params:\n",
    "\n",
    "        max_lag: int\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    acf_list = [acf(x, i, unbiased=unbiased) for i in range(max_lag)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    lags = range(max_lag)\n",
    "    ax.plot(lags, acf_list, 'bo-', markersize=4, linewidth=1)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "    ax.set_xlabel('Lag')\n",
    "    ax.set_ylabel('Autocorrelation')\n",
    "    ax.set_title('Autocorrelation Function over Lag')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_ylim(-1.2, 1.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return acf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47da6ce2-3f21-49c9-8f53-9ef3d075c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_hist(x, bins='fd', base=e): \n",
    "    r\"\"\"\n",
    "        Compute the probability distribution from histogram. Returns the probability and edges (from the hist). \n",
    "\n",
    "        Params: \n",
    "        \n",
    "        x: array-like input data\n",
    "        bins: see numpy documentation. fd: Freedman Diaconis estimator\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    edges = np.histogram_bin_edges(x, bins=bins)\n",
    "    counts, _ = np.histogram(x, bins=edges)\n",
    "    total = counts.sum()\n",
    "\n",
    "    if total == 0: \n",
    "        return np.array([1.0]), edges\n",
    "\n",
    "    p = counts.astype(float) / total\n",
    "\n",
    "    return p, edges\n",
    "\n",
    "def joint_p_hist(x, y, bins='fd', base=e, edge_x=None, edge_y=None):\n",
    "    r\"\"\"\n",
    "        Compute the joint probability distribution P_xy from histogram.\n",
    "\n",
    "        Params: \n",
    "\n",
    "        x, y: array_like input data\n",
    "        edge_x, edge_y: predefined edges of hist_x and hist_y. \n",
    "            Specify edges to ensure consistency. \n",
    "    \"\"\"\n",
    "\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "\n",
    "    if edge_x is None: \n",
    "        edge_x = np.histogram_bin_edges(x, bins=bins)\n",
    "    if edge_y is None: \n",
    "        edge_y = np.histogram_bin_edges(y, bins=bins)\n",
    "\n",
    "    counts, _, _ = np.hisogram2d(x, y, bins=(edge_x, edge_y))\n",
    "    total = counts.sum()\n",
    "\n",
    "    if total == 0: \n",
    "        return np.array([[1.0]], edge_x, edge_y)\n",
    "\n",
    "    p = counts.astype(float) / total\n",
    "\n",
    "    return p, edge_x, edge_y\n",
    "\n",
    "def H_from_p(p, base=e): \n",
    "    r\"\"\"\n",
    "        Compute the shannon entropy given a probability distribution p. \n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    p = p[p > 0]\n",
    "\n",
    "    if p.size == 0: \n",
    "        return 0.0\n",
    "        \n",
    "    return -np.sum(p * (np.log(p) / np.log(base)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54c92819-1a26-4714-b8d5-61cb895a91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delay_embed(time_series, embedding_dimension, time_delay):\n",
    "    r\"\"\"\n",
    "        Delay-embed a 1-dimensional seqeunce (time series). Returns R^{N-t(d-1) x d} delay embedding. \n",
    "\n",
    "        time_series: array-like input data\n",
    "        embedding_dimension (d), time_delay (tau): delay-embedding parameters. \n",
    "\n",
    "        Original 1-d data transformed as [[x_0, x_t, ..., x_t(d-1)], ....]\n",
    "    \"\"\"\n",
    "    N = len(time_series)\n",
    "    num_vecs = N - (embedding_dimension - 1) * time_delay\n",
    "    embedded = np.zeros((num_vecs, embedding_dimension))\n",
    "    for i in range(num_vecs):\n",
    "        for j in range(embedding_dimension):\n",
    "            embedded[i, j] = time_series[i + j * time_delay]\n",
    "\n",
    "    # delay_matrix = num_vecs**(-0.5)*embedded\n",
    "    \n",
    "    return embedded\n",
    "\n",
    "def rolling_cor(x, y, dim, delay): \n",
    "    r\"\"\"\n",
    "        Compute the rolling correlation between two delay-embedded time series. \n",
    "    \"\"\"\n",
    "    x_embed = delay_embed(x, dim, delay)\n",
    "    y_embed = delay_embed(y, dim, delay)\n",
    "    cors = []\n",
    "    for i in range(len(x_embed)): \n",
    "        cor_mat = np.corrcoef(x_embed[i], y_embed[i])\n",
    "        cors.append(cor_mat[0,1])\n",
    "\n",
    "    return np.array(cors)\n",
    "\n",
    "# Old --------\n",
    "def shannon_entropy(x, bins='fd', base=e, from_probs=False): \n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    if from_probs: \n",
    "        p = x/x.sum()\n",
    "    else: \n",
    "        edges = np.histogram_bin_edges(x, bins=bins)\n",
    "        counts, _ = np.histogram(x, bins=edges)\n",
    "        p = counts / counts.sum()\n",
    "\n",
    "    p = [p > 0]\n",
    "    H = -np.sum(p * (np.log(p) / np.log(base)))\n",
    "\n",
    "    return H\n",
    "\n",
    "def joint_entropy(x, y, bins='fd', base=e, from_probs=False):\n",
    "    x, y = np.asarray(x, dtype=float), np.asarray(y, dtype=float)\n",
    "\n",
    "    if prob_probs: \n",
    "        p_xy = x / x.sum()\n",
    "    else: \n",
    "        edges_x = np.histogram_bin_edges(x, bins=bins)\n",
    "        edges_y = np.histogram_bin_edges(y, bins=bins)\n",
    "        counts, _, _ = np.histogram2d(x, y, bins=[edges_x, edges_y]) \n",
    "        p_xy = counts / counts.sum()\n",
    "\n",
    "    p_xy = p_xy.flatten()\n",
    "    p_xy = p_xy[p_xy > 0]\n",
    "\n",
    "    H_xy = -np.sum(p_xy * (np.log(p_xy) / np.log(base)))\n",
    "    \n",
    "    return Hxy\n",
    "\n",
    "def conditional_entropy(x, y, bins='fd', base=e, from_probs=False): \n",
    "    H_xy = join_entropy(x, y, bins=bins, base=base, from_probs=from_probs)\n",
    "    H_y = shannon_entropy(y, bins=bins, base=base, from_probs=from_probs)\n",
    "\n",
    "    return H_xy - H_y\n",
    "\n",
    "def mutual_info(x, y, bins='fd', base=e, from_probs=False): \n",
    "    H_x = shannon_entropy(x, bins=bins, base=base, from_probs=from_probs)\n",
    "    H_y = shannon_entropy(y, bins=bins, base=base, from_probs=from_probs)\n",
    "    H_xy = join_entropy(x, y, bins=bins, base=base, from_probs=from_probs)\n",
    "\n",
    "    return H_x + H_y - H_xy\n",
    "\n",
    "    \n",
    "# mutual information estimate based on historgram\n",
    "def ami(x, max_lag, bins='fd', base=e):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    N = x.size\n",
    "    lags = np.arange(1, max_lag+1)\n",
    "    ami = np.empty_like(lags, dtype=float)\n",
    "\n",
    "    Hx = shannon_entropy(x, bins=bins, base=base)\n",
    "\n",
    "    for idx, tau in enumerate(lags): \n",
    "        x1, x2 = x[:-tau], x[tau:]\n",
    "        \n",
    "    return lags, ami\n",
    "# ------------\n",
    "\n",
    "def delayPCA(time_series, delay_dim, delay, pca_dim): \n",
    "    ts_emb = delay_embed(time_series, delay_dim, delay)\n",
    "    pca = PCA(pca_dim).fit(ts_emb)\n",
    "    ts_delaypca = pca.transform(ts_emb) \n",
    "\n",
    "    return ts_delaypca, pca\n",
    "\n",
    "def project_pca(data, pca, numcomps=2): \n",
    "    x_proj = (data-pca.mean_) @ pca.components_[0:numcomps].T\n",
    "    interval = np.arange(len(x_proj))\n",
    "    if numcomps == 2: \n",
    "        plt.scatter(x_proj[:,0], x_proj[:,1], c=interval, cmap='viridis')\n",
    "        plt.xlabel(\"PC 1\"); plt.ylabel(\"PC 2\")\n",
    "    elif numcomps == 3: \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(x_proj[:,0], x_proj[:,1], x_proj[:,2], c=interval, cmap='viridis', **scatter_kw)\n",
    "        ax.set_xlabel(\"PC 1\"); ax.set_ylabel(\"PC 2\"); ax.set_zlabel(\"PC 3\")\n",
    "    plt.title(f\"Projection onto first {numcomps} PCs\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "        \n",
    "    return x_proj\n",
    "\n",
    "def update_delayPCA(oldata, newobs, dim_delay, delay, dim_pca): \n",
    "    # make sure dim_pca is less than dim_delay\n",
    "    # this is a horrible piece of code; remind myself to fix this\n",
    "    assert dim_delay > dim_pca, \"PCA dim must be less than delay dim\"\n",
    "    \n",
    "    pca = PCA(dim_pca)\n",
    "    oldata_emb = delay_embed(oldata, dim_delay, delay)\n",
    "    old_pca = pca.fit(oldata_emb) \n",
    "    \n",
    "    newdata = np.hstack((oldata, newobs))\n",
    "    newdata_emb = delay_embed(newdata, dim_delay, delay)\n",
    "    new_pca = pca.fit(newdata_emb)\n",
    "    newdata_delaypca = new_pca.transform(newdata_emb)\n",
    "    newdata_oldbasis = old_pca.transform(newdata_emb)\n",
    "    \n",
    "    return old_pca, new_pca, newdata_delaypca, newdata_oldbasis\n",
    "\n",
    "def return_data_stats(data, dim, delay, r): \n",
    "    mean, std, ent, cordim = np.mean(data), np.std(data), approx_entropy(data, dim, delay, r), cor_dim(data, dim, delay, r)\n",
    "    print(f\"Data Mean: {mean} Data Stdev: {std} \\nData Entropy: {ent} Data Correlation Dim: {cordim}\")\n",
    "\n",
    "    return mean, std, ent, cordim\n",
    "\n",
    "def pca_diff_stats(pca1, pca2, ncomps): \n",
    "    angdiff = cossim(pca1.components_[:ncomps], pca2.components_[:ncomps])\n",
    "    meandiff = np.abs(pca1.mean_ - pca2.mean_)\n",
    "    print(f\"Mean Difference: {meandiff} Cosine Similarity: {angdiff}\")\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(pca1.explained_variance_, label=f'{pca1} Eigenvalues')\n",
    "    plt.plot(pca2.explained_variance_, label=f'{pca2} Eigenvalues')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return angdiff, meandiff\n",
    "\n",
    "def compare_pcas(pca1, pca2, k, X1=None, X2=None):\n",
    "    from scipy.linalg import subspace_angles\n",
    "    U1, U2 = pca1.components_[:k].T, pca2.components_[:k].T\n",
    "    ang = subspace_angles(U1, U2)\n",
    "    subspace_dist = np.linalg.norm(np.sin(ang))\n",
    "    var_gap = (pca1.explained_variance_ratio_[:k].cumsum() -\n",
    "               pca2.explained_variance_ratio_[:k].cumsum())[-1]\n",
    "    overlap_sv = np.linalg.svd(U1.T @ U2, compute_uv=False)\n",
    "\n",
    "    results = dict(\n",
    "        principal_angles_deg=np.degrees(ang),\n",
    "        grassmann_dist=subspace_dist,\n",
    "        cum_variance_gap=var_gap,\n",
    "        overlap_singular_vals=overlap_sv,\n",
    "    )\n",
    "    if X1 is not None and X2 is not None:\n",
    "        results[\"recon_MSE_1to2\"] = pca_cross_recon_loss(pca1, pca2, X2)\n",
    "        results[\"recon_MSE_2to1\"] = pca_cross_recon_loss(pca2, pca1, X1)\n",
    "    return results\n",
    "\n",
    "def pca_recon_loss(X, pca, k): \n",
    "    Z = pca.transform(X)\n",
    "    Z[:, k:] = 0\n",
    "    X_hat = pca.inverse_transform(Z)\n",
    "\n",
    "    mse = np.mean((X-X_hat)**2)\n",
    "    per_sample = ((X-X_hat)**2).mean(axis=1)\n",
    "    per_feature = ((X-X_hat)**2).mean(axis=0)\n",
    "    var_j = X.var(axis=0, ddof=0)\n",
    "    r2 = 1.0 - per_feature/var_j\n",
    "\n",
    "    return mse, r2, X_hat\n",
    "\n",
    "# needs to be fixed\n",
    "def pca_cross_recon_loss(X, pca_enc, pca_dec, k): #also use it two compare subspace alignment\n",
    "    # k number of prin. comp.\n",
    "    B_enc, mu_enc = pca_enc.components_[:k].T, pca_enc.mean_\n",
    "    B_dec, mu_dec = pca_dec.components_[:k].T, pca_dec.mean_\n",
    "\n",
    "    Z_enc = (X - mu_enc) @ B_enc\n",
    "\n",
    "    U, _, Vt = np.linalg.svd(B_enc.T @ B_dec, full_matrices = False)\n",
    "    R = U @ Vt\n",
    "\n",
    "    Z_dec = Z_enc @ R\n",
    "\n",
    "    X_hat = Z_dec @ B_dec.T + mu_dec\n",
    "\n",
    "    mse = np.mean((X-X_hat)**2)\n",
    "    per_sample = ((X-X_hat)**2).mean(axis=1)\n",
    "    per_feature = ((X-X_hat)**2).mean(axis=0)\n",
    "    var_j = X.var(axis=0, ddof=0)\n",
    "    r2 = 1.0 - per_feature/var_j\n",
    "\n",
    "    return mse, r2, X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3274bf75-5e17-4652-9069-c6b78cef445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nearest_nbr(data): \n",
    "#     n, d = data.shape\n",
    "#     M_idx = np.empty(N, dtype=int)\n",
    "#     M_dist = np.empty(N, dtype=float)\n",
    "#     for i in range(N):\n",
    "#         best_dist = np.inf\n",
    "#         best_idx = -1\n",
    "#         x_i = data[i]\n",
    "#         for j in range(N): \n",
    "#             if i == j: \n",
    "#                 continue\n",
    "#             dist = np.dot(x_i - data[j], x_i - data[j])\n",
    "#             if dist < best_dist: \n",
    "#                 best_dist = dist\n",
    "#                 best_idx = j\n",
    "#         M_idx[i] = best_idx\n",
    "#         M_dist[i] = best_dist ** 0.5\n",
    "\n",
    "#     return dist, M_idx \n",
    "\n",
    "# def nearest_nbr(X): \n",
    "#     N, d = X.shape\n",
    "#     D = np.full((N,N), np.inf)\n",
    "#     for i in range(N): \n",
    "#         for j in range(N): \n",
    "#             if i == j: \n",
    "#                 continue \n",
    "#             else: \n",
    "#                 D[i][j] = np.sqrt(np.dot(X[i]-X[j], X[i]-X[j])) \n",
    "                \n",
    "#     M_idx = np.argmin(D, axis=1)\n",
    "#     D_min = D[np.arange(N), M_idx]\n",
    "#     return D_min, M_idx\n",
    "\n",
    "def nearest_nbr(X): \n",
    "    diff = X[:, None, :] - X[None, :, :]\n",
    "    D2 = np.sum(diff*diff, axis=-1)\n",
    "\n",
    "    np.fill_diagonal(D2, np.inf)\n",
    "\n",
    "    idx_nn = np.argmin(D2, axis=1)\n",
    "    dists = np.sqrt(D2[np.arange(X.shape[0]), idx_nn])\n",
    "\n",
    "    return dists, idx_nn\n",
    "    \n",
    "def FalseNN(data, max_dim, delay, Atol, Rtol): # false nearest neighbor used to determine optimal embedding dimension\n",
    "    # assert len(data) > \n",
    "    data = np.asarray(data).ravel()\n",
    "    stdev = np.std(data)\n",
    "    if stdev == 0: \n",
    "        raise ValueError(\"Zero Standard Deviation\")\n",
    "\n",
    "    fnn_ptg = []\n",
    "    \n",
    "    for m in range(1, max_dim): \n",
    "        # emb_m = delay_embed(data, m, delay)\n",
    "        emb_m1 = delay_embed(data, m+1, delay)\n",
    "        emb_m = emb_m1[:, :-1]  \n",
    "\n",
    "        d_m, idx_nn = nearest_nbr(emb_m)\n",
    "\n",
    "        new_coord, new_coord_nn = emb_m1[:, -1], emb_m1[idx_nn, -1]\n",
    "        rel_growth = np.abs(new_coord - new_coord_nn) / d_m\n",
    "        cond_rel = rel_growth > Rtol\n",
    "\n",
    "        d_m1 = np.linalg.norm(emb_m1 - emb_m1[idx_nn], axis=1)\n",
    "        cond_abs = (d_m1/stdev) > Atol\n",
    "\n",
    "        fnn_ptg.append(100.0 * np.mean(cond_rel | cond_abs))\n",
    "        \n",
    "\n",
    "    return np.array(fnn_ptg)\n",
    "\n",
    "def lyap_exp(series, dim, delay, theiler_window=0, horizon=100, fs=1.0): \n",
    "    \"\"\"\n",
    "        Calculate the largest Laypunov exponent of a time series using Rosenstein. \n",
    "\n",
    "        Params-----\n",
    "\n",
    "        series (1d array_like): 1d input time series data\n",
    "        dim, delay (int): delay embedding params\n",
    "        theiler_window (int): window to exclude temporally close neighbors. \n",
    "    \"\"\"\n",
    "    traj = delay_embed(series, dim, delay)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "072779a8-637b-4489-a161-7a5cdec3fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLP(nn.Module): \n",
    "    def __init__(self, num_input, num_hidden, num_output): \n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(num_hidden)): \n",
    "            if i==0: \n",
    "                layers.append(nn.Linear(num_input, num_hidden[i]))\n",
    "            else: \n",
    "                layers.append(nn.Linear(num_hidden[i-1], num_hidden[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.hidden = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(num_hidden[-1], num_output)\n",
    "        \n",
    "    def forward(self,x): \n",
    "        x = self.hidden(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
